{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1y8Fsi7JmpmfO24CPGKOSYVQIPb430Fvv",
      "authorship_tag": "ABX9TyPgw9PvMUAsk0TuCQnaenCq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "33813c0ee7644f56ab28d5e09320a703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6eb6344d5a3c412583f39d4d6af80c57",
              "IPY_MODEL_e5ca13f863c24959be3c69c4398936c3",
              "IPY_MODEL_8ab2070137a748b987b7ef8375c8f9ea"
            ],
            "layout": "IPY_MODEL_007364fccf0545efb862481ec0be44af"
          }
        },
        "6eb6344d5a3c412583f39d4d6af80c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9954a16e679847b797fe5c68f32a9115",
            "placeholder": "​",
            "style": "IPY_MODEL_e922d9ea09bd4a5ca9fe62afa771d578",
            "value": "100%"
          }
        },
        "e5ca13f863c24959be3c69c4398936c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9984c2254064e0b9c017142eaa7e9a5",
            "max": 102530333,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06b7cee4b6094553bfaf4bce7bca43db",
            "value": 102530333
          }
        },
        "8ab2070137a748b987b7ef8375c8f9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d104c108ece41c8bf0c0b7103976572",
            "placeholder": "​",
            "style": "IPY_MODEL_cf04da9f40f446738327a537513777cf",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 245MB/s]"
          }
        },
        "007364fccf0545efb862481ec0be44af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9954a16e679847b797fe5c68f32a9115": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e922d9ea09bd4a5ca9fe62afa771d578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9984c2254064e0b9c017142eaa7e9a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06b7cee4b6094553bfaf4bce7bca43db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7d104c108ece41c8bf0c0b7103976572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf04da9f40f446738327a537513777cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZakNasz/Deep-Learning-in-Intelligent-Video-Analytics-and-Computer-Vision-5-day-workshop/blob/main/zakwan_assignment2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aZRz1QDi5ipx"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Transforms to the Data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n"
      ],
      "metadata": {
        "id": "cEMlIJRa5opk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Data\n",
        "\n",
        "# Set train and valid directory paths\n",
        "\n",
        "# dataset = '/content/drive/My Drive/01. TEACHING/MACHINE_VISION/code/fruit_dataset'\n",
        "dataset = '/content/drive/MyDrive/fruit_dataset1'\n",
        "\n",
        "train_directory = os.path.join(dataset, 'train')\n",
        "test_directory = os.path.join(dataset, 'validation')\n",
        "\n",
        "# Batch size\n",
        "batchSize = 32\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_directory))\n",
        "print(num_classes)\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
        "\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
        "# idx_to_class = {v: k for k, v in data['train'].class_to_idx.items()}\n",
        "# print(idx_to_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "528hEliz5qL3",
        "outputId": "3eaa3b11-992c-44ae-b558-7078793767fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FebSlwVF5r5_",
        "outputId": "c9fed271-abe2-4222-f7a1-ba17e0a94523"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 840\n",
              "    Root location: /content/drive/MyDrive/fruit_dataset1/train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               RandomResizedCrop(size=(256, 256), scale=(0.8, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
              "               RandomRotation(degrees=[-15.0, 15.0], interpolation=nearest, expand=False, fill=0)\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               CenterCrop(size=(224, 224))\n",
              "               ToTensor()\n",
              "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "# valid_data_size = len(data['valid'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "train_data_loader = DataLoader(data['train'], batch_size=batchSize, shuffle=True)\n",
        "# valid_data_loader = DataLoader(data['valid'], batch_size=batchSize, shuffle=True)\n",
        "test_data_loader = DataLoader(data['test'], batch_size=batchSize, shuffle=True)"
      ],
      "metadata": {
        "id": "yp3WfHvN5tR4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_size, test_data_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypL9Hwwm5ulK",
        "outputId": "1313e35e-d019-491b-8e34-c6a730eb81b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(840, 323)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (3,32,32)"
      ],
      "metadata": {
        "id": "6aw06PWH5v-C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################\n",
        "#                  Create Dataloader                     #\n",
        "#######################################################\n",
        "\n",
        "# Turn train and test custom Dataset's into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "trainloader = DataLoader(dataset=data['train'], # use custom created train Dataset\n",
        "                                     batch_size=4, # how many samples per batch?\n",
        "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "testloader = DataLoader(dataset=data['test'], # use custom created test Dataset\n",
        "                                    batch_size=4, \n",
        "                                    num_workers=0, \n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyoksec05xVe",
        "outputId": "f9a60135-759b-4c72-ae32-60d94d0d2016"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "840\n",
            "323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################\n",
        "# DEFINE YOUR OWN MODEL\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
        "print(model)\n",
        "model.eval()\n",
        "\n",
        "num_ftrs = model.fc.in_features \n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model.eval = nn.Linear(num_ftrs, 4)\n",
        "\n",
        "\n",
        "\n",
        "#######################\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "33813c0ee7644f56ab28d5e09320a703",
            "6eb6344d5a3c412583f39d4d6af80c57",
            "e5ca13f863c24959be3c69c4398936c3",
            "8ab2070137a748b987b7ef8375c8f9ea",
            "007364fccf0545efb862481ec0be44af",
            "9954a16e679847b797fe5c68f32a9115",
            "e922d9ea09bd4a5ca9fe62afa771d578",
            "d9984c2254064e0b9c017142eaa7e9a5",
            "06b7cee4b6094553bfaf4bce7bca43db",
            "7d104c108ece41c8bf0c0b7103976572",
            "cf04da9f40f446738327a537513777cf"
          ]
        },
        "id": "ah9vfwle50tO",
        "outputId": "09358978-c3e1-40bb-adf0-2a74af60c0d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33813c0ee7644f56ab28d5e09320a703"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              "  (eval): Linear(in_features=2048, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ],
      "metadata": {
        "id": "p_bq134s53ba"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train the model for 10 epochs\n",
        " \n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfvs4unM564U",
        "outputId": "4210bf34-f6cb-4673-811f-d517c21b6548"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 1.2534, Accuracy: 67.1429%, \n",
            "\t\tValidation : Loss : 0.2651, Accuracy: 92.8793%, Time: 646.7558s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.6218, Accuracy: 80.0000%, \n",
            "\t\tValidation : Loss : 0.1064, Accuracy: 95.9752%, Time: 16.8984s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.5801, Accuracy: 80.4762%, \n",
            "\t\tValidation : Loss : 0.0929, Accuracy: 97.8328%, Time: 16.7681s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.5726, Accuracy: 82.6190%, \n",
            "\t\tValidation : Loss : 0.1306, Accuracy: 95.9752%, Time: 16.6815s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.4042, Accuracy: 87.8571%, \n",
            "\t\tValidation : Loss : 0.0988, Accuracy: 96.9040%, Time: 16.6558s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.3598, Accuracy: 86.6667%, \n",
            "\t\tValidation : Loss : 0.1263, Accuracy: 95.6656%, Time: 16.7601s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.3917, Accuracy: 86.6667%, \n",
            "\t\tValidation : Loss : 0.1354, Accuracy: 96.2848%, Time: 16.8119s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.3215, Accuracy: 89.8810%, \n",
            "\t\tValidation : Loss : 0.0991, Accuracy: 96.5944%, Time: 16.8213s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.3294, Accuracy: 89.5238%, \n",
            "\t\tValidation : Loss : 0.0977, Accuracy: 96.9040%, Time: 16.7811s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.2647, Accuracy: 90.9524%, \n",
            "\t\tValidation : Loss : 0.1620, Accuracy: 95.0464%, Time: 16.8000s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "oSVFixyJ588a",
        "outputId": "ae692970-bdfb-43a9-960a-3e71257d5d9e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Analyze the accuracy curve\n",
        "\n",
        "plt.plot(history[:,2:4])\n",
        "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0,1)\n",
        "# plt.savefig('cifar10_accuracy_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "TfdgTob959Sq",
        "outputId": "829fa686-361b-427b-c8a2-8061978457a7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "-qvzt2zX5-y4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "AC5mgF-U6AMi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(size=224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "    ])"
      ],
      "metadata": {
        "id": "rmerM4NbBJQ3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['tomato', 'watermelon','pumpkin','durian']"
      ],
      "metadata": {
        "id": "ZnaxrOITBTRK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    #     rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Apply transforms to the input image.\n",
        "    input_tensor = transform(frame)\n",
        "    # Add the batch dimension.\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "    input_batch = input_batch.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        output = model(input_batch)\n",
        "        end_time = time.time()\n",
        "    # Get the softmax probabilities.\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    # Check the top 5 categories that are predicted.\n",
        "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "    \n",
        "    cv2.putText(frame, f\"{top5_prob[0].item()*100:.3f}%\", (15, (1)*30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    cv2.putText(frame, f\"{categories[top5_catid[0]]}\", (160, (1)*30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    print(categories[top5_catid[0]], top5_prob[0].item())\n",
        "    cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "CfXrKZBJ6B0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQkqZPa66Dlz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}